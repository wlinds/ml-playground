{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean functions 2024 | Perceptron-from-scratch task\n",
    "\n",
    "A perceptron with a single output neuron can be used to determine whether an n-dimensional Boolean function is linearly separable.\n",
    "\n",
    "A Boolean function is defined by a set of inputs. In this task, you implement a computer program that determines whether a Boolean function is linearly separable or not, using a perceptron with activation function $g(b)=sgn(b)$, where\n",
    "\n",
    "$$b = \\sum\\limits_{j=1}^n w_j x_j - \\theta$$\n",
    "\n",
    "[See Eq. (5.9) in the course book for the case n = 2 (arXiv)](https://arxiv.org/abs/1901.05639).\n",
    "\n",
    "Using this program, sample an n-dimensional Boolean function randomly and determine whether it is linearly separable.\n",
    "\n",
    "Do this $10^4$ times for n = 2, 3, 4 and 5 dimensions, and save the number of linearly separable Boolean functions found.\n",
    "\n",
    "The learning rules for the weights $wj$ and threshold $\\theta$ are\n",
    "\n",
    "$$\\delta w_j^{(\\mu)} = \\eta(t^{(\\mu)} - O^{(\\mu)})x_j^{(\\mu)},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\delta \\theta^{(\\mu)} = -\\eta(t^{(\\mu)} - O^{(\\mu)})$$\n",
    "\n",
    "See also Eq. (5.18)\n",
    "\n",
    "For each boolean function, train the perceptron for 20 training epochs (update parameters once for every input-output pair) using a learning rate $\\eta = 0.05$.\n",
    "\n",
    "Initialize the weights from a normal distribution with mean 0 and varance 1/n, and initialize the threshold to 0.\n",
    "\n",
    "Be sure to not count the same Boolean function twice. This can, for example, be done by adding every sampled Boolean function to a list and excluding the function if it comes up a second time.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementing a solution in Python\n",
    "\n",
    "|    |    |\n",
    "| -- | -- |\n",
    "|$\\delta w_j^{(\\mu)}$ | change in the j:th weight for the μ:th training example |\n",
    "|$\\eta$ | learning rate |\n",
    "|$t^{(\\mu)}$ | target (correct) output for the μ:th training example |\n",
    "|$O^{(\\mu)}$ | the actual output produced by the perceptron for the μ:th training example |\n",
    "|$x_j^{(\\mu)}$ | j:th input feature for the μ:th training example |\n",
    "\n",
    "In the Perceptron class, this is implemented as:\n",
    "```\n",
    "self.w += self.eta * (t - O) * x\n",
    "```\n",
    "\n",
    "Here, self.w is the weight vector, self.eta is η, t is the target output, O is the perceptron's output, and x is the input vector.\n",
    "\n",
    "The threshold is very similarly updated:\n",
    "\n",
    "```\n",
    "self.theta -= self.learning_rate * (t - o)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Generating random boolean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def random_bool(n):\n",
    "    X = np.array(list(product([0, 1], repeat=n)))\n",
    "    y = np.random.randint(0, 2, size=2**n)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of possible functions grows exponentially ($2^{(2^n)}$)\n",
    "\n",
    "- Takes an integer n as input, representing the number of input variables (or dimensions).\n",
    "- Creates a numpy array X with all possible combinations of n binary values (0 or 1) using itertools.product() for finding the Cartesian Product\n",
    "- Generates a random binary vector y of length $2^n$, representing the function's output for each input combination.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 1]]),\n",
       " array([0, 1, 1, 0]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_bool(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, X is our input data  with 4 samples and two features, while y is our binary label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Creating a perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, n_inputs, epochs=20):\n",
    "        self.weights = np.random.normal(0, np.sqrt(1/n_inputs), n_inputs)\n",
    "        self.theta = 0\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = 0.05\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (np.dot(X, self.weights) - self.theta > 0).astype(int)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            for x, t in zip(X, y):\n",
    "                o = self.predict(x)\n",
    "                self.weights += self.learning_rate * (t - o) * x\n",
    "                self.theta -= self.learning_rate * (t - o)\n",
    "        return np.all(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Perceptron instance is initialized with random weights from a normal distribution, a threshold of 0, training epochs of 20, and a learning rate of 0.05.\n",
    "\n",
    "The `predict` method is used during training to calculate the dot product of inputs and weights with subtracted threshold. It returns 1 if the result is positive.\n",
    "\n",
    "The `train` method iterates over epochs, calls the prediction and adjusts weights and theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_linearly_separable(X, y, epochs=20):\n",
    "    perceptron = Perceptron(X.shape[1], epochs=epochs)\n",
    "    is_separable = perceptron.train(X, y)\n",
    "    return is_separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] [0 1 0 0]\n",
      "\n",
      "is_linearly_separable(X, y)=True\n"
     ]
    }
   ],
   "source": [
    "X, y = random_bool(2)\n",
    "\n",
    "print(X, y)\n",
    "print(f\"\\n{is_linearly_separable(X, y)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make sure we set our sample rate ($10^4$) and avoid duplicate function checks which is likely to be happening due to the *stochastic* `random_bool()`\n",
    "\n",
    "#### 2. Iterative training over n_dimensions \n",
    "\n",
    "We do this by nesting loops, yeah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_dimensions, n_samples = 10**4, epochs=20):\n",
    "    results = {}\n",
    "    for n in n_dimensions:\n",
    "        separable_count, seen_functions = 0, set()\n",
    "        for i in range(n_samples):\n",
    "            X, y = random_bool(n)\n",
    "            function_hash = hash(tuple(y))\n",
    "            if function_hash not in seen_functions:\n",
    "                seen_functions.add(function_hash)\n",
    "                if is_linearly_separable(X, y, epochs):\n",
    "                    separable_count += 1\n",
    "        results[n] = separable_count\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly separable in Dimension 2: 12/10000\n",
      "Linearly separable in Dimension 3: 95/10000\n",
      "Linearly separable in Dimension 4: 243/10000\n",
      "Linearly separable in Dimension 5: 1/10000\n"
     ]
    }
   ],
   "source": [
    "dimensions = [2,3,4,5]\n",
    "results = main(dimensions)\n",
    "\n",
    "for n, count in results.items():\n",
    "    print(f\"Linearly separable in Dimension {n}: {count}/10000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The perceptron drastically drops in performance past the 4th dimension. I ran the code a couple of times and often I found 0 possible linear separations. Only once did I find a single positive. But it's to no surprise given we only sample 10,000 (without replacement).\n",
    "\n",
    "There's no guarantee to find *all* linearly separable functions unless we completely search the entire space.\n",
    "If we really wanted to find all, we should consider removing sample randomization and explore a more systematic subset of the function space.\n",
    "\n",
    "For R <= 4 the Perceptron class does a fairly good job.\n",
    "\n",
    "What if we increase our sample rate and our training epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly separable in Dimension 5: 0/40,000\n"
     ]
    }
   ],
   "source": [
    "results = main([5], 40_000, 80)\n",
    "for n, count in results.items():\n",
    "    print(f\"Linearly separable in Dimension {n}: {count}/40,000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never lucky. What about the lower dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly separable in Dimension 2: 14/40,000\n",
      "Linearly separable in Dimension 3: 104/40,000\n",
      "Linearly separable in Dimension 4: 879/40,000\n"
     ]
    }
   ],
   "source": [
    "results = main([2,3,4], 40_000, 80)\n",
    "for n, count in results.items():\n",
    "    print(f\"Linearly separable in Dimension {n}: {count}/40,000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "With the initial hyperparameters tuned to 20 epochs and 1,000 input samples, this perceptron successfully found some, but not all, linear separations of boolean functions in lower dimensions. However, when trained on 40,000 samples and 80 epochs, we succesfully identified all valid 2- and 3-dimensional separations and half of the 4-dimensional ones. As there are $2(2^4)$ (65,536) 4-dimensional boolean functions and $2(2^5)$ (4,294,967,296) 5-dimensional ones, we shouldn't expect any better results without refactoring the `random_bool`, or brute forcing over $2(2^5)$ samples, hoping to get lucky."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
